---
title: "METABRIC: RNA SEQ CLUSTERING ANALYSIS"
author: "Sid Gurajala" 
date: "1/1/2021"
output: html_document
---


<!--- Begin styling code. --->
<style type="text/css">
/* Whole document: */
body{
  font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
  font-size: 12pt;
}
h1.title {
  font-size: 38px;
  text-align: center;
}
h4.author {
  font-size: 18px;
  text-align: center;
}
h4.date {
  font-size: 18px;
  text-align: center;
}
</style>
<!--- End styling code. --->

# INTRODUCTION 

  The Molecular Taxonomy of Breast Cancer International Consortium (METABRIC)
dataset contains expression, clinical profiles, copy number variants and 
single nucleotide polymorphism data sequenced from over 2000 primary breast 
cancer tumors. Originally published in [Nature in 2012](https://pubmed.ncbi.nlm.nih.gov/22522925/), the METABRIC consortium has 
become one of the most comprehensive characteriziations of breast cancer 
genetics and physiology, and has feature extensively in publications as both
a reference and a dataset of interest. Breast cancer is currently the second 
most common cancer for women in the US, and in 2020 over 250,000 new cases 
of breast cancer were reported with over 40,000 people dying over the course 
of the disease. It is estimated that about 1 in 8 US women will develop
breast cancer over her lifetime. The importance of understanding the underlying 
genetics of breast cancer in patients therefore cannot be understated. This 
project seeks to apply two types of popular RNA-seq analyses to METABRIC 
data to better understand expression profiles in patients. Both kmeans 
clustering and hierarchical clustering will be used to explore how suites of 
genes partition themselves in accordance with expression profiles. 

[Metabric data can be found here](https://ega-archive.org/studies/EGAS00000000098) 

```{r, echo=FALSE, include=FALSE}
#Preliminary data reading in and filtering 
metabric <- read.csv("data/METABRIC_RNA_mutation.csv")

#FILTERING DATA 

library(dplyr)
names_metabric <- names(metabric)
#filters names_metabric for RNA expression columns
names_metabric <- names_metabric[seq(32,520)]
#filters metabric to exclude other cause deaths and selects expression columns
metabric_df <- na.omit(metabric %>% 
                        filter(death_from_cancer != "Died of Other Causes") %>% 
                        select(patient_id, 
                               death_from_cancer, 
                               all_of(names_metabric)))

#Reformats death_from_cancer category
metabric_df$death_from_cancer <- gsub("Living", 0, 
                                      metabric_df$death_from_cancer)
metabric_df$death_from_cancer <- gsub("Died of Disease", 1, 
                                      metabric_df$death_from_cancer)
metabric_df$death_from_cancer <- as.numeric(metabric_df$death_from_cancer)
```

# PCA 

    One of the biggest challenges in dealing with biological data sets is the large
number of dimensions, which can often hinder computationally intensive analyses.
A means to pare down data into essential parts that are representative of 
the variance found in the whole data set is PCA, or prinicipal component 
analysis. PCA is a linear transformation orthogonal to current data, and seeks
to assign data to principal components where the the first principal component
captures the most variance from the original data set and further components 
have lower and lower values of explained variance. I have applied PCA to 
RNA-seq data to see if dimensionality reduction can be achieved. 

```{r PCA and PC scores Plot}
#PCA

#drop nas and invert so genes are rows
metabric_mx <- as.matrix(na.omit(metabric_df[, 1:491]))
colnames_mt_mx <- metabric_mx[, 1]
metabric_mx <- t(metabric_mx[, 2:491])
#creating pca object
metabric_pca <- prcomp(metabric_mx[2:490, ], scale = TRUE)

#exploring PCA visually
library(ggplot2)
pc_scores <- metabric_pca$x
pc_scores_plot <- as.data.frame(pc_scores) %>%
                    ggplot(aes(x = PC1, y = PC2)) +
                    geom_point() +
                    labs(title = "PC Scores for RNA seq z scores data") +
                    theme_bw()
                  
pc_scores_plot
```

    
  While the Principal component scores for these data show quite a lot of 
variance, there is some structure to the data. While PCA may not be entirely
useful for this data, you can see a few clusters around the middle of the 
plot. Further visualizations should help elucidate the efficiency for PCA 
in reducing the dimensions of this data set. 

```{r PCA screeplot and Biplot, message=FALSE, warning=FALSE}
#creating biplot
library(ggbiplot)
metabric_biplot <- ggbiplot(metabric_pca) +
                    theme_bw() +
                    scale_color_manual(labels = c("Died from Disease",
                                                  "Living"), 
                                       values = c("darkorange", "purple")) 
#drops text labels crowding graph                    
metabric_biplot$layers[[3]] <- NULL

#creating screeplot df 
var_explained <- (metabric_pca$sdev) ^ 2 / sum((metabric_pca$sdev) ^ 2)
var_explained <- var_explained[1:9]
screeplot_pca_df <- data.frame(PC = paste0("PC", 1:9),
                               Variance_Explained = (var_explained))
#making screeplot 
screeplot_pca <- screeplot_pca_df %>%  
                  ggplot(aes(x = PC, y = Variance_Explained)) +
                  geom_point(size = 5) +
                  labs("Screeplot of PC 1 - 10 for METABRIC expression data",
                       x = "Principal Component",
                       y = "Variance Explained") +
                  theme_bw()

screeplot_pca
#display biplot
metabric_biplot

```

  Both of these plots demonstarte that PCA may not be an effective means of 
decomposition for this data. The biplot shown states that both principal 
components explain relatively little of the data (6.8% for PC1 and 6.3% for 
PC2). The screeplot also shows that additional principal components do not 
effectively explain variance in the dataset. While PCA has the potential to be 
an effective tool for increasing the manageability of the size of data sets,
these visualizations make it clear that it cannot be used as a tool for 
these specific data. 

# K MEANS CLUSTERING - MODEL SELECTION

  K means clustering is to date one of the most popular unsupervised machine 
learning algorithims. It uses a predetermined number (k) clusters to partition 
data into groups more homogenous than initial data. Data points are assigned to
clusters by calculating by reducing the in cluster sum of squares, the squared
euclidean distance between each data point and the centroid (the center of each 
cluster around which other data points are organized). The algorithm starts with 
randomly selected centroids and then iterates calculations to locate the optimal 
position for the centroids. K means clustering seeks to minimize the distance 
within a cluster while maximizing the distance between clusters. The major 
requirement of k means clustering is beforehand knowledge of the optimal number 
of clusters before using the algorithm to partition data. While selecting the 
number of clusters is more an art than a science, there are several 
computational tools that can help visualize k means clustering and inform 
decision making. 

## SSE 
  
  The Sum of Squared Errors is a measurement of the distance of data points 
from the nearest cluster. Minimizing SSE can help determine the optimal 
number of clusters and thereby maximize the k means algorithm's efficiency in 
data partioning. The formula for SSE is:  
$$ SSE = \sum_{i = 1}^{n} (X_i - \bar{X}) ^ 2$$
  Below I calculate the SSE for k = 2 to k = 20 and visualize the results to 
determine the optimal k value. 

```{r Sum of Squared Errors, warning = FALSE, message = FALSE}
#for loop calculating sse for k 1 to 20 clusters
metabric_sse <- vector()
for (i in 2:20) {
  metabric_sse[i] <- sum(kmeans(metabric_mx[2:490, ], centers = i)$withinss)
}

#creates df of metabric sse values
metabric_sse_df <- na.omit(data.frame(sse = metabric_sse,
                                       clusters = 1:20))
#Creates SSE plot 
sse_plot <- ggplot(metabric_sse_df, aes(x = clusters, y = sse)) +
              geom_point() +
              geom_smooth() +
              theme_bw() +
              labs(title = "SSEs for 2 to 20 clusters using kmeans",
                    y = "Sum of Squared Error") 
sse_plot
```

  The key to this visualization is to look for the "elbow:" the point at which
the addition of more clusters doesn't significantly reduce the sum of squared 
error. Based on this plot, the elbow appears to be around 7-9 clusters. Further
tests should better elucidate the best number of clusters.

## Average Silhouette Width

  The silhouette width of kmeans clusters measures the euclidean distance 
between two clusters. The objective of silhouette width measurements is to 
maximize the distance, resulting in a high silhouette coefficient. This in 
turn results in distinct clusters with little overlap. Below I have measured
the average silhouette width for k = 2 to k = 20 clusters. 

```{r Silhouette Width}
library(cluster)
#for loop calculating silhouette width for 1 to 20 clusters 
metabric_sil <- vector()
for (i in 2:20) {
  k_one_to_twenty <- kmeans(metabric_mx[2:490, ], centers = i, nstart = 25, 
                            iter.max = 20)
  ss <- silhouette(k_one_to_twenty$cluster, dist(metabric_mx[2:490, ]))
  metabric_sil[i] <- mean(ss[, 3])
}
#creates silhoutte width df
metabric_sil_df <- na.omit(data.frame(AvgSilhouetteWidth = metabric_sil,
                                      cluster = 1:20))
#plots silhouette width
sil_plot <- ggplot(metabric_sil_df, aes(x = cluster, y = AvgSilhouetteWidth)) +
              geom_point() +
              geom_smooth(se = FALSE) +
              labs(title = "Average Silhouette Width for 2 to 20 clusters",
                   y = "Average Silhouette Width") +
              theme_bw()
sil_plot
```

  The silhouette plot indicates that six clusters is the optimal number of 
clusters for this analysis, with a range between 5 and 8 clusters all having 
high silhouette values. 

## Calinski-Harabasz Criterion
  
  The Calinksi-Harabasz index is rooted in the sum of squared error within 
and between clusters. Maximizing the index involves using iterative kmeans 
clustering algorithms, and I carried out the Calinksi-Harabasz over 100 
iterations from k = 2 to k = 20 clusters. 

```{r Calinski-Harabasz Index}
## Calinksy Criterion 
library(vegan)
#calculates calinsky index for 1 to 20 clusters 
metabric_calinksi <- cascadeKM(metabric_mx[2:490, ], 2, 20, iter = 100)
#Creates heatmap and index plot
plot(metabric_calinksi, sortg = TRUE, grpmts.plot = TRUE)
```

  While the max Calinski-Harabasz index value is at k = 2 clusters, two clusters
often result in k means cluster models that are too generalizable and segregates
points into clusters they may not be homogenous with. Index values between 
k = 3 to k = 6 are also relatively high, pointing to a k value between these 
two numbers as a good choice for kmeans cluster analysis. 

## Gap Statistic 

  The gap statistic informs on the best cluster number by comparing the 
log within cluster sum of squares with the expected log sum of squares under 
a null reference distribution. The optimal number of clusters is selected by
maximizing the gap between the data set's within cluster sum of squares and the
null reference distribution. Below I have carried out this analysis with k = 1 
to k = 20 clusters over 100 iterations.

```{r Gap Statistic}
## Gap Statistic
set.seed(13)
#calculates Gap statistics 
metabric_gap <- clusGap(metabric_mx[2:490, ], kmeans, 20, B = 100, 
                        verbose = interactive())
#plot for gap statistics
plot(metabric_gap, main = "Gap statistic across 20 clusters")
```

  While the max gap statistic value is at k = 20, this model would likely be 
far too specific and would lack robustness with other data sets. The inflection
point of this plot seems to be at around 6-8 clusters, with values around this 
range also having a high gap value. The cumulative suggestion of these analyses
suggests that anywhere between 4 and 7 clusters would be appropriate for these
data. 

# K MEANS CLUSTERING - CLUSTER ANALYSIS 

  Given previous tests, the optimal number of clusters seems to be around 4.
Below I have arranged several different analyses of clustered expression data,
starting with partioning with k = 4.

```{r Clustering}
# Clustering
set.seed(20)
kClust <- kmeans(metabric_mx[2:490, ], centers = 4,
                 nstart = 1000, iter.max = 20)
metabric_clust <- kClust$cluster
```

## Calculating Centroids and Analyzing Correlations

  An after the fact method to validate cluster number selection is to calculate
cluster centroids and correlate them with each other. Cluster centroids
displaying a high degree of correlation ( > 0.85) is suggestive of unspecific
clusters with similar profiles in euclidean space. Below I have made a function
to calculate centroids and have computed correlations between them.

```{r Calculating Centroids and Centroid Correlation}
## Finding centroids; takes col means
cluster_centroid <- function(num, data, clusters) {
  x <- (clusters == num)
  colMeans(data[x, ])
}

#calculates centroids 
metabric_clust_centroids <- sapply(levels(factor(metabric_clust)), 
                                   cluster_centroid,
                                   metabric_mx[2:490, ],
                                   metabric_clust)
#calculates correlation between centroids 
cor(metabric_clust_centroids)
```

  As expected, cluster centroids do not correlate very highly with one another,
with a maximum correlation value of - 0.531. This validates the choice of k = 4
clusters, enabling us to move onto deeper analyses. 

## Visualizing Cluster Expression for 6 patients 

  A point of interest is understanding how these clusters behave across 
different conditions. Oftentimes this is analyzed between two different 
treatment groups or time points within an experiment. In the case of these data
it is valuable to tie the difference between cluster expression to the variance
found in the population of breast cancer patients. I have carried this out below
using six separate patients. 

```{r 6 Patient Cluster Expression}
#adding gene name as column and cluster# to metabric_mx_2 
metabric_mx_2 <- as.data.frame(metabric_mx[2:490, ])
names_mx_2 <- rownames(metabric_mx_2)
rownames(metabric_mx_2) <- seq(1:489)
metabric_df_2 <- metabric_mx_2 %>% 
                  mutate(gene = names_mx_2) %>% 
                  mutate(cluster = as.vector(metabric_clust))
#Extracting expression data for 6 patients 
patient_col_names <- c("Expression", "gene", "cluster", "Patient")
patient1_df <- metabric_df_2 %>% 
  select(1418, gene, cluster) %>% 
  mutate(patient = "Patient 1") 
colnames(patient1_df) <- patient_col_names
```

```{r Patients 2 - 6, include = FALSE}
patient2_df <- metabric_df_2 %>% 
  select(1423, gene, cluster) %>% 
  mutate(patient = "Patient 2")
colnames(patient2_df) <- patient_col_names
patient3_df <- metabric_df_2 %>% 
  select(1422, gene, cluster) %>% 
  mutate(patient = "Patient 3")
colnames(patient3_df) <- patient_col_names
patient4_df <- metabric_df_2 %>% 
  select(1421, gene, cluster) %>% 
  mutate(patient = "Patient 4")
colnames(patient4_df) <- patient_col_names
patient5_df <- metabric_df_2 %>% 
  select(1420, gene, cluster) %>% 
  mutate(patient = "Patient 5")
colnames(patient5_df) <- patient_col_names
patient6_df <- metabric_df_2 %>% 
  select(1419, gene, cluster) %>% 
  mutate(patient = "Patient 6")
colnames(patient6_df) <- patient_col_names
```
 
 Using similar methods for Patients 2 through 6, I compiled the expression 
per cluster into patients_df and plot the expression per cluster per patient
below. 

```{r Combining Patients 1-6 and Plotting}
#combining patient dfs 
patientsdf <- rbind(patient1_df, patient2_df, patient3_df,
                     patient4_df, patient5_df, patient6_df)
#making plot
kmeans_patients <- ggplot(data = patientsdf, aes(x = cluster, y = Expression)) +
                    geom_bar(stat = "identity", fill = "#FF6666") +
                    facet_wrap(~Patient) +
                    labs(title = "Expression by kmeans cluster for 6 Patients",
                         x = "Cluster",
                         y = "Normalized Expression") +
                    theme_bw()

kmeans_patients 
```

  This figure displays the variation in expression patterns between clusters. 
By and large, patient cluster expression profiles are similar and follow 
general patterns; with cluster 4 having the most negative normalized
expression and cluster 1 having the highest normalized expression. 
The differences between these clusters reflect population variances, 
demonstrating how differences in tumor types, individual physiology and 
genetics is shown in clustered expression profiles. 

## Extracting Core Genes 

  A final thing we can do with clustering analysis of RNA seq is to extract core 
genes, i.e genes that best align with the core of the cluster. This is done 
by analyzing the correlation between each gene in a cluster and the cluster's 
core. I have done this below and assembled a table of the five genes from each 
cluster with the highest correlation scores. 

```{r Extracting Core Genes Cluster 1}
library(reshape)
#melt data to long form
metabric_clust_melt <- melt(metabric_clust_centroids)
#assign colnames 
colnames(metabric_clust_melt) <- c("sample", "cluster", "value")
#subset melted data frame
metabric_core_1 <- metabric_clust_melt %>% 
                    filter(cluster == "1")
#assign metabric_mx with only genes
metabric_mx_2 <- metabric_mx[2:490, ]

#pull out cluster 1
metabric_clust_1 <- metabric_mx_2[metabric_clust == 1, ]
#function that calculates correlation score for cluster 1
correlation_score_1 <- function(n) {
  cor(n, metabric_core_1$value)
}
#calculates cluster1 scores 
clust1_scores <- apply(metabric_clust_1, 1, correlation_score_1)
#identifies top 5 highest correlated genes 
core_clust_1_genes <- tail(sort(clust1_scores))
```

  Using similar methods I determined core genes for clusters 2 - 4 and assembled
into the table seen below. 

```{r Core Genes Clusters 2 - 4, include = FALSE}
## Isolating core genes from rest of clusters
metabric_core_2 <- metabric_clust_melt %>% 
                    filter(cluster == "2")
metabric_clust_2 <- metabric_mx_2[metabric_clust == 2, ]
correlation_score_2 <- function(n) {cor(n, metabric_core_2$value)}
clust_2_scores <- apply(metabric_clust_2, 1, correlation_score_2)
core_clust_2_genes <- tail(sort(clust_2_scores))

metabric_core_3 <- metabric_clust_melt %>% 
                    filter(cluster == "3")
metabric_clust_3 <- metabric_mx_2[metabric_clust == 3, ]
correlation_score_3 <- function(n) {cor(n, metabric_core_3$value)}
clust_3_scores <- apply(metabric_clust_3, 1, correlation_score_3)
core_clust_3_genes <- tail(sort(clust_3_scores))

metabric_core_4 <- metabric_clust_melt %>% 
                    filter(cluster == "4")
metabric_clust_4 <- metabric_mx_2[metabric_clust == 4, ]
correlation_score_4 <- function(n) {cor(n, metabric_core_4$value)}    
clust_4_scores <- apply(metabric_clust_4, 1, correlation_score_4)
core_clust_4_genes <- tail(sort(clust_4_scores))
```

```{r Assembling Core Gene Table}
#Combining core genes from clusters 1 through 4
core_genes_df <- data.frame("cluster 1 genes" = names(core_clust_1_genes),
                            "cluster 1 scores" = core_clust_1_genes,
                            "cluster 2 genes" = names(core_clust_2_genes),
                            "cluster 2 scores" = core_clust_2_genes,
                            "cluster 3 genes" = names(core_clust_3_genes),
                            "cluster 3 scores"= core_clust_3_genes,
                            "cluster 4 genes" = names(core_clust_4_genes),
                            "cluster 4 scores" = core_clust_4_genes)
#assigning row names of 1 to 6 for genes 
rownames(core_genes_df) <- c(1:6)
#display core_genes_df
core_genes_df
```

  Understanding how core genes align with kmeans cluster cores can help cut down
on noise in clusters - especially helping distinguish between genes that help
define a cluster well and genes that clutter a cluster with low correlation 
scores. Extracting these core genes also enables identification of genes of 
interest that resemble each other in expression profiles. This opens up an 
avenue of possibilities connecting machine learning with biological pathways
and systems. 

# HIERARCHICAL CLUSTERING 

  Hierarchical clustering is a seperate unsupervised clustering algorithm that
does not rely on a priori knowledge of optimal cluster number. It operates by 
building a hierarchy of clusters instead of partioning data into a predefined 
number of clusters, with the optimal number of clusters determined a posteriori 
with the help of dendrogram like tree visualizations. The hierarchical 
clustering algorithm also uses distance measurement between data points, in this 
case the normalized expression scores of genes. Below I use R's inbuilt
hierarchical clustering function hclust() and visualize. 

```{r }
#HIERARCHICAL CLUSTERING 

#calculating gene distance for each row
metabric_gene_dist <- dist(metabric_mx_2)
#using hclust function on gene distance
metabric_hclust <- hclust(metabric_gene_dist, method = "complete")
#dendrogram of genes clustered
plot(metabric_hclust, labels = FALSE, main = "Gene Clustering Dendogram")
abline(h = 62, lwd = 2)
```

  The line represents an eyeballed optimal number of clusters, which I have
determined are around 6 clusters. This number can be used to partition the 
expression data for further analyses. 

## Cluster Expression Profiles for 6 Patients 

  A similar analysis to the K means clustering can be carried out with 
hierarchical clustered data. The dendrogram of hierarchically clustered METABRIC
data suggests that six clusters would be appropriate. Below I use R's cutree()
function to forcibly partition data and extract expression data per cluster for 
six patients. 

```{r Hierarchical Clusters Patient 1}
#seperating data by cluster and turning it into a tibble
metabric_hc_clusters <- as.vector(cutree(metabric_hclust, k = 6))
#adding gene name as column and cluster# to metabric_mx_2 
metabric_df_3 <- as.data.frame(metabric_mx_2) %>% 
                    mutate(gene = names_mx_2) %>% 
                    mutate(cluster = metabric_hc_clusters) 
#Extracting expression data for 6 patients 
patient_1_df <- metabric_df_3 %>% 
                  select(1418, gene, cluster) %>% 
                  mutate(patient = "Patient 1") 
colnames(patient_1_df) <- patient_col_names
```

  Using similar methods I calculated expression data per cluster for patients 
2 through 6 and display the results in the bar plot below. 
```{r Hierarchical Clusters Patient 2 - 6, include = FALSE}
patient_2_df <- metabric_df_3 %>% 
                  select(1423, gene, cluster) %>% 
                  mutate(patient = "Patient 2")
colnames(patient_2_df) <- patient_col_names
patient_3_df <- metabric_df_3 %>% 
                  select(1422, gene, cluster) %>% 
                  mutate(patient = "Patient 3")
colnames(patient_3_df) <- patient_col_names
patient_4_df <- metabric_df_3 %>% 
                  select(1421, gene, cluster) %>% 
                  mutate(patient = "Patient 4")
colnames(patient_4_df) <- patient_col_names
patient_5_df <- metabric_df_3 %>% 
                  select(1420, gene, cluster) %>% 
                  mutate(patient = "Patient 5")
colnames(patient_5_df) <- patient_col_names
patient_6_df <- metabric_df_3 %>% 
                  select(1419, gene, cluster) %>% 
                  mutate(patient = "Patient 6")
colnames(patient_6_df) <- patient_col_names
```

```{r Assembling patients_df and displaying expression profiles}
#combining patient dfs 
patients_df <- rbind(patient_1_df, patient_2_df, patient_3_df,
                     patient_4_df, patient_5_df, patient_6_df)
#making plot
patient_plot <- ggplot(data = patients_df, aes(x = cluster, y = Expression)) +
                  geom_bar(stat = "identity", fill = "#FF6666") +
                  facet_wrap(~Patient) +
                  labs(
                    title = "Expression by hierarchical cluster for 6 Patients",
                    x = "Cluster",
                    y = "Normalized Expression") +
                  theme_bw()

patient_plot
```

  This plot shows similar features to the kmeans clustered gene expression 
profiles. Specifically, clusters 1 and 2 have generally the highest normalized 
expression while cluster 5 has the lowest range and cluster 4 has the lowest
normalized expression. This also reflects natural variation in the population,
demonstrating how these analyses must be informed by background genetics and 
clinical profiles. 

## Heatmap Visualization

  One of the most popular ways to visualize RNA-seq data is through heatmap
based hierarchical clustering. This provides an accessible, distinctive way to
distinguish clusters within expression data as well as an attractive foundation 
for future analyses connecting genes with correlated profiles. Below I've 
rendered a heatmap with hierarchically clustered expression data. 

```{r}
#Making heatmap
library(gplots)
hc_dist <- cor(t(metabric_mx_2), method = "pearson")
hc_hr <- hclust(as.dist(1 - hc_dist), method = "complete")

#heatmap.2 of hc_dist 
heatmap.2(hc_dist,
          Rowv = as.dendrogram(hc_hr),
          Colv = as.dendrogram(hc_hr),
          scale = "row",
          margins = c(2,2),
          cexCol = 0.7,
          labRow = FALSE,
          labCol = FALSE,
          main = "Hierarchically clustered Heatmap",
          trace = "none")
```

  This heatmap is a convenient visualization of what has been observed earlier
within the dendogram - namely multiple small clusters of expression profiles 
that cosegregate with other clusters to fit within a larger tree. This can be 
seen specifically along the top right hand corner of the heatmap, which 
designates several clusters of interest side by side. 

# CONCLUSION

  These analyses represent a relevant and powerful intersection between popular
unsupervised machine learning algorithms and genetic expression profiles 
underlying a pertinent data set exploring a real world problem. The data 
gathered via clustering analyes can often be used to inform our knowledge of 
biological systems, and can reveal connections between suites of genes that 
humans would never think to draw. Expanding on these algorithms and adjusting
modeling to better suit the types of data encountered can provide powerful 
insights into everything from gene regulation to disease outcome. 
